{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current cell code gotten from https://www.kaggle.com/stevengolo/classification-and-localization\n",
    "import xml.etree.ElementTree as etree\n",
    "ANNOTATION_DIR = '.\\VOC2012\\Annotations'\n",
    "def extract_xml_annotation(filename):\n",
    "    # function gotten from https://www.kaggle.com/stevengolo/classification-and-localization\n",
    "    \"\"\"Parse the xml file\n",
    "    :param filename: str\n",
    "    \"\"\"\n",
    "    z = etree.parse(filename)\n",
    "    objects = z.findall('./object')\n",
    "    size = (int(float(z.find('.//width').text)), int(float(z.find('.//height').text)))\n",
    "    fname = z.find('./filename').text\n",
    "    dicts = [{obj.find('name').text: [int(float(obj.find('bndbox/xmin').text)),\n",
    "                                      int(float(obj.find('bndbox/ymin').text)),\n",
    "                                      int(float(obj.find('bndbox/xmax').text)),\n",
    "                                      int(float(obj.find('bndbox/ymax').text))]}\n",
    "             for obj in objects]\n",
    "    return {'size': size, 'filename': fname, 'objects': dicts}\n",
    "\n",
    "annotations = []\n",
    "for filename in sorted(os.listdir(ANNOTATION_DIR)):\n",
    "    annotation = extract_xml_annotation(os.path.join(ANNOTATION_DIR, filename))\n",
    "    \n",
    "    new_objects = []\n",
    "    for obj in annotation['objects']:\n",
    "        new_objects.append(obj)\n",
    "    \n",
    "    if len(new_objects) == 1:\n",
    "        annotation['class'] = list(new_objects[0].keys())[0]\n",
    "        annotation['bbox'] = list(new_objects[0].values())[0]\n",
    "        annotation.pop('objects')\n",
    "        annotations.append(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(annotations)\n",
    "# class = 1 means person, class = 0 means not person\n",
    "def changeToNotPerson(className):\n",
    "    if (className == 'person'):\n",
    "        className = '1'\n",
    "    else:\n",
    "        className = '0'\n",
    "    return className\n",
    "df['class'] = df['class'].apply(changeToNotPerson)\n",
    "df['class'] = pd.to_numeric(df['class'], downcast='integer')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only needs to run once\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "IMAGE_SHAPE = (224, 224)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "images = []\n",
    "for filename in df['filename']:\n",
    "    print('reading file ' + filename)\n",
    "    img = image.load_img('./VOC2012/JPEGImages/' + filename, target_size=IMAGE_SHAPE)\n",
    "    img = image.img_to_array(img)\n",
    "    img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "    preprocess_input(img)\n",
    "    # normalize image\n",
    "    images.append(img)\n",
    "images = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, np.array(df['class']), test_size=0.15, random_state=42)\n",
    "# del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing and compiling of the model\n",
    "from tensorflow.keras.applications import EfficientNetB3, MobileNetV2, NASNetMobile, DenseNet169\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input, Activation, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "neurons = 64\n",
    "lr = 0.01\n",
    "outputs = 1\n",
    "\n",
    "base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=IMAGE_SHAPE + (3,), pooling='max')\n",
    "print(base_model.summary())\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = Input(shape=IMAGE_SHAPE + (3,))\n",
    "x = base_model(inputs, training=False)\n",
    "# A Dense classifier with a single unit (binary classification)\n",
    "x_next = Dense(100, activation='relu')(x)\n",
    "x_out = Dense(1, activation='sigmoid')(x_next)\n",
    "model = Model(inputs, x_out)\n",
    "model.compile(optimizer=Adam(),\n",
    "                        loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCheck = ModelCheckpoint('bestModel_checkpoint2.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "hist = model.fit(X_train, y_train, \n",
    "              epochs=5, \n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[ModelCheck]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads best model from the training phase\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('bestModel_checkpoint2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f, ax = plt.subplots()\n",
    "ax.plot([None] + hist.history['accuracy'], '--')\n",
    "ax.plot([None] + hist.history['val_accuracy'])\n",
    "# Plot legend and use the best location automatically: loc = 0.\n",
    "ax.legend(['Train acc', 'Validation acc'], loc = 0)\n",
    "ax.set_title('Training/Validation acc per Epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with different images from the webcam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.efficientnet import decode_predictions\n",
    "import numpy as np\n",
    "IMAGE_SHAPE=(224,224)\n",
    "img = image.load_img('WIN_20201015_13_50_45_Pro.jpg', target_size=IMAGE_SHAPE)\n",
    "img = image.img_to_array(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "# img = preprocess_input(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prediction = model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting Keras model into a Tensorflow.js model\n",
    "from tensorflowjs.converters import save_keras_model\n",
    "save_keras_model(model, './EN3_PersonNoPerson_classifier_TfJS')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('Deep_Learning': conda)",
   "language": "python",
   "name": "python37764bitdeeplearningcondab49442486b034519931e1963468738e5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}